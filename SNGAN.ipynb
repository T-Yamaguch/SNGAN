{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of SNGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/T-Yamaguch/SNGAN/blob/main/SNGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8Tzp4G4_cKs",
        "outputId": "5d2eccb4-bce4-43b5-9c51-9db674c2613e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCaY7uIeBtcK",
        "outputId": "0327c97d-ae79-43b3-d1cd-8a73b0306d55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Oct 13 08:19:34 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   45C    P8    10W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o8kDrWDjAJy",
        "outputId": "b0e9b3fd-4718-4838-8ce9-0b51f66aebe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "!mkdir data_faces && wget https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip \n",
        "\n",
        "import zipfile\n",
        "\n",
        "with zipfile.ZipFile(\"celeba.zip\",\"r\") as zip_ref:\n",
        "  zip_ref.extractall(\"data_faces/\")\n",
        "\n",
        "import os\n",
        "root = 'data_faces/img_align_celeba'\n",
        "img_list = os.listdir(root)\n",
        "print(len(img_list))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-13 08:19:34--  https://s3-us-west-1.amazonaws.com/udacity-dlnfd/datasets/celeba.zip\n",
            "Resolving s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)... 52.219.24.185\n",
            "Connecting to s3-us-west-1.amazonaws.com (s3-us-west-1.amazonaws.com)|52.219.24.185|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1443490838 (1.3G) [application/zip]\n",
            "Saving to: ‘celeba.zip’\n",
            "\n",
            "celeba.zip          100%[===================>]   1.34G  53.4MB/s    in 27s     \n",
            "\n",
            "2020-10-13 08:20:01 (51.3 MB/s) - ‘celeba.zip’ saved [1443490838/1443490838]\n",
            "\n",
            "202599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3JkBM0L4_c9H"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.layers import Input, Concatenate, Conv2D, \\\n",
        "MaxPooling2D, Activation, ReLU, LeakyReLU, UpSampling2D, BatchNormalization, \\\n",
        "Dropout, Dense, Flatten, Add, LayerNormalization, GaussianNoise, Reshape, Lambda\n",
        "from keras.regularizers import l2\n",
        "\n",
        "class up_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(up_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.up = UpSampling2D((2,2))\n",
        "    self.noise = GaussianNoise(0.2)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.up(x)\n",
        "    x = self.noise(x)\n",
        "    return x\n",
        "\n",
        "class res_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(res_block, self).__init__()\n",
        "    self.conv1 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.conv2 = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm1 = BatchNormalization(trainable=True)\n",
        "    self.norm2 = BatchNormalization(trainable=True)\n",
        "    self.act1 = LeakyReLU()\n",
        "    self.act2 = LeakyReLU()\n",
        "    self.add = Add()\n",
        "\n",
        "  def call(self, x):\n",
        "    y = self.conv1(x)\n",
        "    y = self.norm1(y)\n",
        "    y = self.act1(y)\n",
        "    y = self.conv2(y)\n",
        "    y = self.norm2(y)\n",
        "    y = self.act2(y)\n",
        "    x = self.add([x, y])\n",
        "    return x\n",
        "\n",
        "class down_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(down_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class conv_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(conv_block, self).__init__()\n",
        "    self.conv = Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.drop = Dropout(0.3)\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.conv(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "class dense_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001), batch_norm = True):\n",
        "    super(dense_block, self).__init__()\n",
        "    self.dense = Dense(filter_num, kernel_regularizer= kernel_regularizer)\n",
        "    self.norm = BatchNormalization(trainable=True)\n",
        "    self.act = LeakyReLU()\n",
        "    self.batch_norm = batch_norm\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.dense(x)\n",
        "    if self.batch_norm == True:\n",
        "      x = self.norm(x)\n",
        "    x = self.act(x)\n",
        "    return x\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E0sM99Z_Zl-"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.python.keras.backend as K\n",
        "from tensorflow.keras.layers import *\n",
        "\n",
        "class SpectralNormalization(tf.keras.layers.Wrapper):\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super(SpectralNormalization, self).__init__(layer, **kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "            self.w = self.layer.kernel\n",
        "            self.u = tf.Variable(\n",
        "            tf.random.normal((tuple([1, self.layer.kernel.shape.as_list()[-1]])), dtype=tf.float32), \n",
        "            aggregation=tf.VariableAggregation.MEAN, trainable=False)\n",
        "        super(SpectralNormalization, self).build()\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        def _l2normalize(v, eps=1e-12):\n",
        "            return v / (K.sum(v ** 2) ** 0.5 + eps)\n",
        "        def power_iteration(W, u):\n",
        "            _u = u\n",
        "            _v = _l2normalize(K.dot(_u, K.transpose(W)))\n",
        "            _u = _l2normalize(K.dot(_v, W))\n",
        "            return _u, _v\n",
        "        w_shape = self.w.shape.as_list()\n",
        "        w_reshaped = K.reshape(self.w, [-1, w_shape[-1]])\n",
        "        _u, _v = power_iteration(w_reshaped, self.u)\n",
        "        sigma = K.dot(_v, w_reshaped)\n",
        "        sigma = K.dot(sigma, K.transpose(_u))\n",
        "        w_bar = w_reshaped / sigma\n",
        "        if training == False:\n",
        "            w_bar = K.reshape(w_bar, w_shape)\n",
        "        else:\n",
        "            with tf.control_dependencies([self.u.assign(_u)]):\n",
        "                 w_bar = K.reshape(w_bar, w_shape) \n",
        "        output = self.layer(inputs)\n",
        "        return output\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(\n",
        "            self.layer.compute_output_shape(input_shape).as_list())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ETSfRk_GuA9W"
      },
      "source": [
        "class downsn_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(downsn_block, self).__init__()\n",
        "    self.convsn = SpectralNormalization(Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer))\n",
        "    self.act = LeakyReLU()\n",
        "    self.pooling = MaxPooling2D((2,2), strides=(2,2))\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.convsn(x)\n",
        "    x = self.act(x)\n",
        "    x = self.pooling(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "class convsn_block(Model):\n",
        "  def __init__(self, filter_num, kernel_size, kernel_regularizer= l2(0.001)):\n",
        "    super(convsn_block, self).__init__()\n",
        "    self.convsn = SpectralNormalization(Conv2D(filter_num, kernel_size, padding = 'same', kernel_regularizer= kernel_regularizer))\n",
        "    self.act = LeakyReLU()\n",
        "    self.drop = Dropout(0.3)\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.convsn(x)\n",
        "    x = self.act(x)\n",
        "    x = self.drop(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "class densesn_block(Model):\n",
        "  def __init__(self, filter_num, kernel_regularizer= l2(0.001)):\n",
        "    super(densesn_block, self).__init__()\n",
        "    self.densesn = SpectralNormalization(Dense(filter_num, kernel_regularizer= kernel_regularizer))\n",
        "    self.act = LeakyReLU()\n",
        "\n",
        "  def call(self, x):\n",
        "    x = self.densesn(x)\n",
        "    x = self.act(x)\n",
        "    return x\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fx2s6XE9_mkK",
        "outputId": "5479f1cf-d6cd-48e0-f40a-03dae2607d21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "class Generator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 1024\n",
        "    self.layer_num = 5\n",
        "    self.res_num = 0\n",
        "    self.latent_num = 8\n",
        "    self.inputs = Input(shape=(self.latent_num)) \n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'generator'\n",
        "    self.kernel_regularizer= None\n",
        "      \n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    final_size = 4*4*self.channel_num\n",
        "    data_size = self.latent_num\n",
        "\n",
        "    # while data_size*64 < final_size:\n",
        "    #   data_size *= 64\n",
        "    #   x = dense_block(data_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = dense_block(final_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Reshape((4, 4, self.channel_num))(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    \n",
        "    for n in range(self.layer_num):\n",
        "      for m in range(self.res_num):\n",
        "        x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num /= 2\n",
        "      x = up_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    for m in range(self.res_num):\n",
        "      x = res_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    # x = conv_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    x = Conv2D(3, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    x = Activation('tanh')(x)\n",
        "    outputs = x\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "g = Generator()\n",
        "g.model().summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"generator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 8)]               0         \n",
            "_________________________________________________________________\n",
            "dense_block (dense_block)    (None, 16384)             212992    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 4, 4, 1024)        0         \n",
            "_________________________________________________________________\n",
            "up_block (up_block)          (None, 8, 8, 512)         13109760  \n",
            "_________________________________________________________________\n",
            "up_block_1 (up_block)        (None, 16, 16, 256)       3278080   \n",
            "_________________________________________________________________\n",
            "up_block_2 (up_block)        (None, 32, 32, 128)       819840    \n",
            "_________________________________________________________________\n",
            "up_block_3 (up_block)        (None, 64, 64, 64)        205120    \n",
            "_________________________________________________________________\n",
            "up_block_4 (up_block)        (None, 128, 128, 32)      51360     \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 128, 128, 3)       2403      \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 128, 128, 3)       0         \n",
            "=================================================================\n",
            "Total params: 17,679,555\n",
            "Trainable params: 17,644,803\n",
            "Non-trainable params: 34,752\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ie8PcTw_nEC",
        "outputId": "77479cfb-fd4b-4e52-b99e-b7a030df18ed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487
        }
      },
      "source": [
        "class Discriminator():\n",
        "  def __init__(self):\n",
        "    self.channel_num = 8\n",
        "    self.layer_num = 5\n",
        "    self.input_shape = (128, 128, 3)\n",
        "    self.inputs = Input(shape=self.input_shape)\n",
        "    self.kernel_size = (5, 5)\n",
        "    self.name = 'discriminator'\n",
        "    self.kernel_regularizer= None\n",
        "\n",
        "  def model(self):\n",
        "    x = self.inputs\n",
        "\n",
        "    x = GaussianNoise(0.2)(x)\n",
        "\n",
        "    filter_num = self.channel_num\n",
        "    for n in range(self.layer_num):\n",
        "      x = downsn_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "      filter_num *= 2\n",
        "\n",
        "    x = convsn_block(filter_num, self.kernel_size, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "    \n",
        "    x = Flatten()(x)\n",
        "    outputs =  Dense(1, kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "    # outputs = Conv2D(1, self.kernel_size, padding = 'same', kernel_regularizer= self.kernel_regularizer)(x)\n",
        "\n",
        "\n",
        "    return Model(inputs = self.inputs, outputs = outputs, name = self.name)\n",
        "\n",
        "d = Discriminator()\n",
        "d.model().summary()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 128, 128, 3)]     0         \n",
            "_________________________________________________________________\n",
            "gaussian_noise_5 (GaussianNo (None, 128, 128, 3)       0         \n",
            "_________________________________________________________________\n",
            "downsn_block (downsn_block)  (None, 64, 64, 8)         616       \n",
            "_________________________________________________________________\n",
            "downsn_block_1 (downsn_block (None, 32, 32, 16)        3232      \n",
            "_________________________________________________________________\n",
            "downsn_block_2 (downsn_block (None, 16, 16, 32)        12864     \n",
            "_________________________________________________________________\n",
            "downsn_block_3 (downsn_block (None, 8, 8, 64)          51328     \n",
            "_________________________________________________________________\n",
            "downsn_block_4 (downsn_block (None, 4, 4, 128)         205056    \n",
            "_________________________________________________________________\n",
            "convsn_block (convsn_block)  (None, 4, 4, 256)         819712    \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 4096)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 4097      \n",
            "=================================================================\n",
            "Total params: 1,096,905\n",
            "Trainable params: 1,096,401\n",
            "Non-trainable params: 504\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyTiix-n_pgj"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.losses import binary_crossentropy, MSE\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import sys\n",
        "import keras.backend as K\n",
        "\n",
        "class GAN():\n",
        "  def __init__(self, \n",
        "               img_size= 128, \n",
        "               code_num = 2048,\n",
        "               batch_size = 16, \n",
        "               train_epochs = 100, \n",
        "               train_steps = 8, \n",
        "               checkpoint_epochs = 25, \n",
        "               image_epochs = 1, \n",
        "               start_epoch = 1,\n",
        "               optimizer = Adam(learning_rate = 1e-4),\n",
        "               n_critics = 8,\n",
        "               gp_lamda = 0.1\n",
        "               ):\n",
        "    \n",
        "    self.batch_size = batch_size\n",
        "    self.train_epochs =  train_epochs\n",
        "    self.train_steps = train_steps\n",
        "    self.checkpoint_epochs = checkpoint_epochs\n",
        "    self.image_epochs = image_epochs\n",
        "    self.start_epoch = start_epoch\n",
        "    self.code_num = code_num\n",
        "    self.img_size = img_size\n",
        "    self.n_critics = n_critics\n",
        "    self.gp_lamda = gp_lamda\n",
        "    \n",
        "    self.gen_optimizer = optimizer\n",
        "    self.disc_optimizer = optimizer\n",
        "\n",
        "    g = Generator()\n",
        "    self.gen = g.model()\n",
        "    \n",
        "    d = Discriminator()\n",
        "    self.disc = d.model()\n",
        "\n",
        "    checkpoint_dir = \"drive/My Drive/SNGAN/checkpoint\"\n",
        "    checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "    checkpoint = tf.train.Checkpoint(gen_optimizer = self.gen_optimizer,\n",
        "                                     disc_optimizer = self.disc_optimizer,\n",
        "                                     gen = self.gen,\n",
        "                                     disc = self.disc,\n",
        "                                     )\n",
        "\n",
        "    self.manager = tf.train.CheckpointManager(checkpoint, directory=checkpoint_dir, max_to_keep=2)\n",
        "\n",
        "    train_image_path = 'data_faces/img_align_celeba'\n",
        "    \n",
        "    self.train_filenames = glob.glob(train_image_path + '/*.jpg') \n",
        "\n",
        "    checkpoint.restore(self.manager.latest_checkpoint)\n",
        "\n",
        "    self.g_history = []\n",
        "    self.d_history = []\n",
        "\n",
        "    self.real_history = []\n",
        "    self.fake_history = []\n",
        "    # self.endec_history = []  \n",
        "\n",
        "  def preprocess_image(self, image):\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [self.img_size, self.img_size] )\n",
        "    image = image*2 /255 - 1  # normalize to [-1,1] range\n",
        "    return tf.cast(image, tf.float32)\n",
        "\n",
        "  def load_and_preprocess_image(self, path):\n",
        "    image = tf.io.read_file(path)\n",
        "    return self.preprocess_image(image)\n",
        "\n",
        "  def dataset(self, paths, batch_size):\n",
        "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(paths)\n",
        "    img_ds = path_ds.map(self.load_and_preprocess_image, num_parallel_calls=AUTOTUNE)\n",
        "    img_ds = img_ds.batch(batch_size)\n",
        "    return img_ds\n",
        "\n",
        "  def image_preparation(self, filenames, batch_size, steps):\n",
        "    img_batch = []\n",
        "    while 1:\n",
        "      random.shuffle(filenames)\n",
        "      for path in filenames:\n",
        "        img_batch.append(path)\n",
        "        if len(img_batch) == steps*batch_size:\n",
        "          imgs = self.dataset(img_batch, batch_size)\n",
        "          img_batch = []\n",
        "          yield imgs\n",
        "\n",
        "  def gan_loss(self, original_outputs, generated_outputs):\n",
        "    real_loss = binary_crossentropy(tf.ones_like(original_outputs), original_outputs)\n",
        "    generated_loss = binary_crossentropy(tf.zeros_like(generated_outputs), generated_outputs)\n",
        "    d_loss = tf.math.reduce_mean(real_loss + generated_loss)\n",
        "    g_loss = tf.math.reduce_mean(binary_crossentropy(tf.ones_like(generated_outputs), generated_outputs))\n",
        "    return d_loss, g_loss\n",
        "\n",
        "\n",
        "  def wasserstein_loss(self, ori_outputs, gen_outputs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def w_gp_loss(self, ori_outputs, gen_outputs, imgs, gen_imgs):\n",
        "    d_loss = -tf.reduce_mean(ori_outputs) + tf.reduce_mean(gen_outputs)\n",
        "    g_loss = -tf.reduce_mean(gen_outputs)\n",
        "\n",
        "    alpha = tf.random.uniform(shape=[self.batch_size, 1, 1, 1], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    inter_sample = alpha * imgs + (1-alpha) * gen_imgs\n",
        "    with tf.GradientTape() as tape_gp:\n",
        "      tape_gp.watch(inter_sample)\n",
        "      inter_score = self.disc(inter_sample)\n",
        "    gp_gradients = tape_gp.gradient(inter_score, inter_sample)\n",
        "    gp_gradients_norm = tf.sqrt(tf.reduce_sum(tf.square(gp_gradients), axis = [1, 2, 3]))\n",
        "    gp = tf.reduce_mean((gp_gradients_norm - 1.0) ** 2)\n",
        "\n",
        "    d_loss += gp * self.gp_lamda\n",
        "    \n",
        "    return d_loss, g_loss\n",
        "\n",
        "\n",
        "  def hinge_loss(self, ori_outputs, gen_outputs):\n",
        "    c = 1E-4 #Gap between D(x) and D(G(x))\n",
        "    d_loss = K.mean(K.relu(c - ori_outputs)) + K.mean(K.relu(c + gen_outputs))\n",
        "    g_loss =  -1 * K.mean(gen_outputs)\n",
        "    return d_loss, g_loss\n",
        "    \n",
        "\n",
        "  def gan_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      self.real_history.append(tf.reduce_mean(ori_outputs))\n",
        "      self.fake_history.append(tf.reduce_mean(gen_outputs))\n",
        "\n",
        "      d_loss, g_loss = self.hinge_loss(ori_outputs, gen_outputs)\n",
        "      # d_loss, g_loss = self.w_gp_loss(ori_outputs, gen_outputs, imgs, gen_imgs)\n",
        "\n",
        "      self.g_temp.append(g_loss)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def g_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      gen_imgs = self.gen(noise, training=True)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=False)\n",
        "      gen_outputs = self.disc(gen_imgs, training=False)\n",
        "\n",
        "      _, g_loss = self.hinge_loss(ori_outputs, gen_outputs)\n",
        "      self.g_temp.append(g_loss)\n",
        "\n",
        "    gradients_of_gen = gen_tape.gradient(g_loss, self.gen.trainable_variables)\n",
        "    self.gen_optimizer.apply_gradients(zip(gradients_of_gen, self.gen.trainable_variables))\n",
        "\n",
        "  def d_train(self, imgs):\n",
        "    noise =tf.random.uniform([self.batch_size, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:\n",
        "      gen_imgs = self.gen(noise, training=False)\n",
        "\n",
        "      ori_outputs = self.disc(imgs, training=True)\n",
        "      gen_outputs = self.disc(gen_imgs, training=True)\n",
        "      \n",
        "      d_loss, _ = self.hinge_loss(ori_outputs, gen_outputs)\n",
        "      self.d_temp.append(d_loss)\n",
        "\n",
        "    gradients_of_disc = disc_tape.gradient(d_loss, self.disc.trainable_variables)    \n",
        "    self.disc_optimizer.apply_gradients(zip(gradients_of_disc, self.disc.trainable_variables))\n",
        "\n",
        "  def visualise_batch(self, s_1, epoch):\n",
        "    gen_img = self.gen(s_1)  \n",
        "    gen_img = (np.array((gen_img+1)*255/2, np.uint8))\n",
        "    fig, axes = plt.subplots(4, 6)\n",
        "    for idx, img in enumerate(gen_img):\n",
        "      p, q = idx//6, idx%6\n",
        "      axes[p, q].imshow(img)\n",
        "      axes[p, q].axis('off')\n",
        "    \n",
        "    save_name = 'drive/My Drive/SNGAN/generated_image/'+'image_at_epoch_{:04d}.png'\n",
        "    plt.savefig(save_name.format(epoch), dpi=100)\n",
        "    plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def loss_vis(self):\n",
        "    plt.plot(self.g_history, 'b', self.d_history, 'r')\n",
        "    plt.title('blue: g  red: d')\n",
        "    plt.savefig('drive/My Drive/SNGAN/loss/gan_loss.png')\n",
        "    plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "    plt.plot(self.fake_history, 'b', self.real_history, 'r')\n",
        "    plt.title('blue: fake  red: real')\n",
        "    plt.savefig('drive/My Drive/SNGAN/loss/D_output.png')\n",
        "    plt.pause(0.1)\n",
        "    plt.close('all')\n",
        "\n",
        "  def update_loss_history(self):\n",
        "    d_batch_loss = sum(self.d_temp)/len(self.d_temp)\n",
        "    g_batch_loss = sum(self.g_temp)/len(self.g_temp)\n",
        "    print ('\\nd loss: {:.4f}, g loss: {:.4f}'.format(d_batch_loss, g_batch_loss))\n",
        "    self.d_history.append(d_batch_loss)\n",
        "    self.g_history.append(g_batch_loss)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "  def __call__(self):\n",
        "    sample_noise =tf.random.uniform([24, self.code_num], minval=0, maxval=1, dtype=tf.dtypes.float32)\n",
        "    image_loader = self.image_preparation(self.train_filenames, self.batch_size, self.train_steps)\n",
        "    self.d_temp = []\n",
        "    self.g_temp = []\n",
        "\n",
        "    for epoch in range(self.start_epoch, self.train_epochs+1):\n",
        "      \n",
        "      start_time = time.time()\n",
        "\n",
        "      print ('\\nepochs {}'.format(epoch))\n",
        "      imgs_ds = next(image_loader)\n",
        "\n",
        "      for steps, imgs in enumerate(imgs_ds):\n",
        "        print(\"\\r\" + 'steps{}'.format(steps+1), end=\"\")\n",
        "        sys.stdout.flush()\n",
        "        self.gan_train(imgs)\n",
        "\n",
        "        # for img in imgs:\n",
        "        #   plt.imshow(img)\n",
        "        #   plt.pause(0.1)\n",
        "\n",
        "        # self.d_train(imgs)\n",
        "\n",
        "        # if steps % self.n_critics == 0:\n",
        "        #   self.g_train(imgs)\n",
        "        \n",
        "      self.update_loss_history()\n",
        "\n",
        "      epoch_time = time.time() - start_time\n",
        "      print (\"epoch time: {:.4f} [sec]\".format(epoch_time))\n",
        "                               \n",
        "      if epoch % self.image_epochs == 0:\n",
        "        self.visualise_batch(sample_noise, epoch)\n",
        "        self.loss_vis()\n",
        "\n",
        "      if epoch % self.checkpoint_epochs == 0:\n",
        "        print ('\\nSaving checkpoint at epoch{}\\n\\n'.format(epoch))\n",
        "        self.manager.save()\n",
        "      \n",
        "if __name__ == '__main__':\n",
        "  a = GAN(img_size = 128,\n",
        "          code_num = 8,\n",
        "          batch_size = 64,\n",
        "          train_epochs = 10000, \n",
        "          train_steps = 128, \n",
        "          checkpoint_epochs = 10, \n",
        "          image_epochs = 1, \n",
        "          start_epoch = 1,\n",
        "          optimizer = Adam(learning_rate = 1e-6),\n",
        "          n_critics = 8,\n",
        "          gp_lamda = 10\n",
        "          )\n",
        "  a()"
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}